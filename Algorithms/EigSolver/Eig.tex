\lab{Algorithms}{Eigenvalue Solvers}{Eigenvalue Solvers}
\objective{Implement the $QR$ algorithm for finding eigenvalues.}
\label{lab:EigSolve}

\section*{Eigenvalues are hard to find}

Finding the eigenvalues of $n \times n$ matrix $A$ means solving the following equation, where $x$ is a nonzero vector and $\lambda$ is a scalar.
\begin{align}
A x &= \lambda x \notag \\
A x - \lambda x &= 0 \notag \\
(A - \lambda I)x  &= 0 \label{eq:singularity}
\end{align}
Since $x$ is nonzero, \eqref{eq:singularity} means $A-\lambda I$ must be singular.
Thus $\det(A - \lambda I) = 0$.  This determinant is often notated $\det(A - \lambda I) = p(\lambda)$ and is called the \emph{characteristic polynomial} of $A$.
The roots of the characteristic polynomial are the eigenvalues of $A$.

If $A$ is $n \times n$, the degree of $p(\lambda)$ is  $n$.
Finding the roots is easy for small $n$, but it becomes difficult or impossible as $n$ increases.
Abel's Theorem  outlines the problem.

\begin{theorem}
{\bf Abel's Impossibility Theorem:} There is no general algebraic solution for solving a polynomial equation of degree $n>4$.
\label{Theorem:Abel}
\end{theorem}

Therefore, there is no method that will exactly find the eigenvalues of an arbitrary matrix.
This is a significant result. In practice it means that we often rely on iterative methods, which converge to the eigenvalues.

\section*{The $QR$ algorithm}

There are many such iterative methods for finding eigenvalues.
We will explore one of the simplest: the $QR$ algorithm.
The following recurrence describes the $QR$ Algorithm in its most basic form:
\begin{equation*}
A_0 = A, \quad A_k = Q_k R_k, \quad A_{k+1} = R_k Q_k
\end{equation*}
where $Q_k, R_k$ is the $QR$ decomposition of $A_k$.
Yes, it's as easy as it looks.
All this algorithm does at each step is find the $QR$ decomposition of $A_k$ and multiply $Q_k$ and $R_k$ together again but in the opposite order.
How does this simple algorithm find the eigenvalues of $A$?

\begin{comment}
\begin{problem}
\label{problem:similarity proof}
Prove that $A_{k+1} \sim A_k$ (where $\sim$ denotes matrix similarity).
Then prove that $A_n \sim A$ for all $n$.
\end{problem}
\end{comment}

Observe that $A_{k+1} \sim A_k$ (where $\sim$ denotes matrix similarity).
Then $A_n \sim A$ for all $n$.
This statement shows that $A_n$ has the same eigenvalues as $A$.
Preservation of eigenvalues is the first important feature that makes the algorithm work.
The other important feature is that each iteration of the algorithm effectively transfers some of the``mass" from the lower to the upper triangle.
Under very general conditions, $A_n$ will converge to a matrix of the form

\begin{equation*}
\label{eq:Schur form}
S =
     \begin{pmatrix}
          S_1 &* & \cdots & * \\
           0     &S_2  &  \ddots & \vdots \\
           \vdots  & \ddots & \ddots & *  \\
           0 & \cdots & 0 & S_m
    \end{pmatrix}
\end{equation*}
where $S_i$ is either a $1 \times 1$ or a $2 \times 2$ matrix.
For most matrices $A$, all the $S_i$ will be $1 \times 1$, so $S$ will be an upper triangular matrix.
In this case, $S$ is called the \emph{Schur form} of $A$.
The eigenvalues of $A$ are on the main diagonal of $S$.

The only case where $S$ is not upper triangular is when $A$ is a real but not symmetric matrix.
In this case, though $A$ is real, it may have complex eigenvalues.
These eigenvalues occur in complex conjugate pairs.
Each of these pairs corresponds to a $2 \times 2$ block in $S$, where the eigenvalues of the $2 \times 2$ block are the complex conjugate pair of eigenvalues of $A$.
In this case, $S$ is called the \emph{real Schur form} of $A$.

\subsection*{Hessenberg preconditioning}

Recall from Lab \ref{lab:Canonical Transformations} that an upper Hessenberg matrix looks like
\[
\begin{pmatrix}
* & * & * & \cdots & * \\
* & * & * & \cdots & * \\
0 & * & * & \cdots&* \\
\vdots & & \ddots & \ddots & \vdots \\
0 & \cdots & 0 & * & *\\
\end{pmatrix}
\]
and that every matrix is similar to an upper Hessenberg matrix.
Hessenberg reduction also preserves eigenvalues.
It is a good idea to reduce to Hessenberg form before continuing with the $QR$ algorithm.
You'll converge to the Schur form faster this way, since Hessenberg matrices are already close to upper triangular.

\begin{problem}
Write a function \li{qrSolver} that implements the QR algorithm as described above. Use the QR decomposition built into \li{scipy.linalg}.
Have your function accept a real-valued $n \times n$ matrix $A$, a number of iterations, and a tolerance number,
and return the corresponding estimate for all the eigenvalues of $A$.
Note that you will need to find the eigenvalues of the $2 \times 2$ $S_i$ directly, whenever there are any.
\end{problem}

Reducing the matrix to Schur form also has the added benefit that each iteration for an $n \times n$ array can be computed in $\mathcal{O} \left( n^2 \right)$ time instead of $\mathcal{O} \left( n^3 \right)$.
One algorithm for computing the QR decomposition of an upper Hessenberg matrix was discussed in Lab \ref{lab:givens}.
This works since $Q$ in the QR factorization of an upper Hessenberg matrix is also upper Hessenberg, the product $R Q$ also turns out to be upper Hessenberg.

\begin{comment}
\begin{problem}
\label{prob:QR_eig_hessenberg}
Write a version of the QR algorithm that performs the QR algorithm by computing the Hessenberg form of a matrix, then computing various QR decompositions of the Hessenberg form of the matrix.
Use your solutions to \ref{prob:hessenberg} (where you computed the Hessenberg form of a matrix) and Problem \ref{prob:givens_hessenberg_modified} to do the necessary computations (where you computed the QR decomposition of a Hessenberg matrix and wrote code for multiplication by $Q$ that works in $\mathcal{O} \left( n^2 \right)$ time).
The solution to Problem \ref{prob:givens_hessenberg_modified} is especially important because it allows the compution of each QR decomposition and each $R Q = \left( Q^T R^T \right)$ in $\mathcal{O} \left( n^2 \right)$ time.
\end{problem}
\end{comment}

\begin{comment}

\begin{problem}
If $A$ is normal, its Schur form is diagonal.
For normal $A$, have your function additionally output the eigenvector corresponding to each eigenvalue.
Hint 1: Test your function on Hermitian and real symmetric matrices; they are both normal.
Hint 2: Your work in Problem \ref{problem:similarity proof} will help.
You have already made all the necessary calculations, you just need to store the information correctly.
\end{problem}

\end{comment}

\begin{problem}
Test your implementation with random matrices.
Try real-valued and symmetric matrices.
Compare your output to the output from the eigenvalue solver.
How many iterations are necessary?
How large can $A$ be?
\end{problem}

Be aware that the algorithm we coded today is a very un-optimized approach.
In practice, the QR algorithm as we have described it here is not generally used.
There is an improved version called the Implicit QR algorithm that iterates much more efficiently.

Further, the $QR$ algorithm is not the only iterative method used to find eigenvalues.
Arnoldi iteration is similar to the $QR$ algorithm but exploits sparsity.
Other methods include the Jacobi method and the Rayleigh quotient method.

As a final note, it is important to remember that eigenvalue solvers can be wrong, particularly for matrices that are ill-conditioned. Convergence is not unconditionally guaranteed. 