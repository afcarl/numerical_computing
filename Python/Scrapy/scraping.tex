\lab{Python}{Scrapy}{Web Scraping}
\objective{Teach students how to scrape websites.}

\section*{Scraping Data}
Scraping data from a website is a way to extract structured or unstructured data from the HTML of a page.

Most websites have APIs that direct people how to take data from the webpage. Often you use the API. You use a scraping tool if you want to extract data from a website that does not have an API or the data you want is not a part of the API. (Like extracting e-mail addresses of users).  Be careful however, because many website prohibit scraping their content.  Before scraping a website, make sure you are legally able to do so.

There is a nice python package called \li{scrapy} that is an application framework for scraping.  The official documentation can be found at \url{http://doc.scrapy.org/en/latest/}. 

Another package is BeautifulSoup.  The documentation can be found at \url{http://www.crummy.com/software/BeautifulSoup/bs4/doc/}

\begin{info}
You will be making and editing python files in this lab, so you will not be using IPython.
\end{info}

\begin{warn}
This lab was written using version 0.16 of scrapy. If you are using a later version the code will not be exactly the same.
\end{warn}

\section*{Setting up}

To begin a project go to the directory where you want to store the code and run
\begin{lstlisting}
scrapy startproject <project name>
\end{lstlisting}
This will create several files. \li{scrapy.cfg} is a configuration file. It will create folder called \li{<project name>} , that contains files \li{items.py}, \li{pipelines.py}, \li{settings.py} and a directory called \li{spiders}. We will use all this files.

\section*{Items}
Items are like python dictionaries with some additional functionality. Go to the \texttt{items.py} found in the  \li{<project name>} directory. Items are labels for the data that you be storing.

\begin{lstlisting}
from scrapy.item import Item, Field

class <project name>Item(Item):
    <item1> = Field()
    <item2> = Field()
    <item3> = Field()
\end{lstlisting}

\section*{Spiders}
Spiders are classes used to scrape data from a group of websites. You will define classes that define the initial list of URLs to download, how to follow the links, and how to parse the contents of the pages into your items objects. There are two main spiders. \li{BaseSpider} and \li{CrawlSpider}. A BaseSpider is used for one website and a CrawlSpider used for crawling multiple websites.

We will start with a BaseSpider. Go in your spider directory and create a new file (the name of the file does not matter). Subcalss BaseSpider as shown below. A BaseSpider has three things. A name that is a unique identifier, \li{start_urls} which is a list of URLS from which the spider will begin to crawl from and parse(), a method which will be called with the Response object at the beginning of each url.

the following is an example.
\begin{lstlisting}
from scrapy.spider import BaseSpider

class DmozSpider(BaseSpider):
    name = "example"
    allowed_domains = ["example.org"]
    start_urls = [
        "http://www.example.org/Computers/",
        "http://www.example.org/books/"
    ]

    def parse(self, response):
        hxs = HtmlXPathSelector(response)
        sites = hxs.select('//td')
        for site in sites:
            title = site.select('a/text()').extract()
            link = site.select('a/@href').extract()
            desc = site.select('text()').extract()
            print title, link, desc
\end{lstlisting}

Next we are going to explain what is in the parse method.

\section*{Parsing}
Scrapy uses a mechanism called selectors to extract data from web pages.
They have three methods.

the \li{select(path)} method returns a list of selectors, each of them representing nodes by the expression given as the argument.This involves knowing a little HTML. HTML has elements whose name is enclosed in <>. So in the code above \li{select('//td')} selects  all the \li{<td>} elements. \li{select('//a/text()')} selects the text inside the \li{<a>} elements. the \li{//div[@class="mine"]} selects all the \li{<div>} elements with the attribute \li{class="mine"}.  

The \li{extract()} method return a string with the data selected by the selector.

The \li{re(string)} returns a list of strings extracted by applying the regular expression given as the argument.

You can test this in a shell by typing in
\begin{lstlisting}
scrapy shell <URL of website>
\end{lstlisting}

try \li{scrapy shell http://www.dmoz.org/} and \li{hxs.select('//title')}. You will get feedback as if you were using the the \li{hxs.select()} in your code. You can try other methods to see what they do./usr/lib/python2.7/site-packages/cssselect/xpath.pyc

\section*{Storing the data}
To store the data we use our items. 

Say we defined our items like so
\begin{lstlisting}
from scrapy.item import Item, Field

from scrapy.item import Item, Field

class SportscrapeItem(Item):
    names = Field()
    numbers = Field()
\end{lstlisting}

And we definded our spider like so.
\begin{lstlisting}
from scrapy.spider import BaseSpider
from scrapy.selector import HtmlXPathSelector

from tutorial.items import DmozItem

class DmozSpider(BaseSpider):
   name = "dmoz"
   allowed_domains = ["dmoz.org"]
   start_urls = [
       "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",
       "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"
   ]

   def parse(self, response):
       hxs = HtmlXPathSelector(response)
       sites = hxs.select('//ul/li')
       items = []
       for site in sites:
           item = DmozItem()
           item['title'] = site.select('a/text()').extract()
           item['link'] = site.select('a/@href').extract()
           item['desc'] = site.select('text()').extract()
           items.append(item)
       return items
\end{lstlisting}

Then I would go to the top directory and run
\begin{lstlisting}
scrapy crawl dmoz -o items.json -t json
\end{lstlisting}
which will generate a file \li{items.json} where all the scraped items are serialized in JSON. You can also store them in csv or xml by changing all the json to csv or xml.

\section*{Crawling the Web}
The spider above will only scrape one page. In order to scrape more pages you will need to inherit from \li{CrawlSpider}. The main difference is you set a list of rules for how you spider clicks on links and you do not over write the parse() method.

\begin{warn}
When using a \li{CrawlSpider} do not overwrite the \li{parse} method.
\end{warn}

For example if I were scrapying the ESPN website for basketball statistics, this code would start of the mainpage and click are the links as specified in the rules. So below I made a\li{rule} list and for each rule I cread a \li{Rule} object. The first varible a takes in is a link extractor. the most common one is the li{SgmlLinkExtractor}. Two of its many attributes it has is \li{allow} and \li{deny}. \li{allow} takes in a list of strings and will click on those links has those words in the url. \li{deny} also takes in a list of strings and will not click on links with those words in the url. \li{deny} has higher precedence than \li{allow}. For a \li{Rule} object you set \li{callback} as a string of the method that parses the pages that you click on with the link extractor.  And than you set \li{follow=True} which means that it will click on those links.

The following code will go to the espn nba statistics website and click on the links that have \li{'statistics/team/_/stat'} in the name but not \li{'sort'} and scrape those webpages.

\begin{lstlisting}
from scrapy.contrib.spiders import CrawlSpider, Rule
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
from scrapy.spider import BaseSpider
from scrapy.selector import HtmlXPathSelector

from sportscrape.items import SportscrapeItem

class EspnSpider(CrawlSpider):
   name = "espn"
   allowed_domains = ["espn.go.com"]
   start_urls = [
       "http://espn.go.com/nba/statistics"
   ]
   rules = (
        Rule(SgmlLinkExtractor(allow=('statistics/team/_/stat',),deny=('sort',)),callback='parse_page',follow=True),
    )

   def parse_page(self, response):
       hxs = HtmlXPathSelector(response)
       sites = hxs.select('//td')
       items = []
       for site in sites:
           item = SportscrapeItem()
           item['numbers'] = site.select('text()').extract()
           item['names'] = site.select('a/text()').extract()
           items.append(item)
       return items
\end{lstlisting}

\section*{Pipelining}
Now you have a spider that crawls the web and dumps all the data into csv or json file. For some applications that will be enough. 

In addition you can cleanse the data or put it into a database via pipelining. In your directory \li{<project name>} you will have a file called \li{pipelines.py}. In this file you can create a class that has three methods \li{process_item(item, spider)}, \li{open_spider(spider)} and \li{close_spider(spider)}. The methods \li{open_spider} and \li{closed_spider} all called when the project is started and ended respectively. The \li{process_item} is called when the \li{parse} method of the spider returns. 

\subsection*{Cleansing}

The following code drops an item if it does not have anything in the data part of the object and returns it otherwise.

\begin{lstlisting}
from scrapy.exceptions import DropItem

class DataPipeline(object):

    def process_item(self, item, spider):
        if item['data']:
            return item
        else:
            raise DropItem("Missing data in %s" % item)
\end{lstlisting}

\subsection*{Adding to a database}
The following code puts the items where both its attributes are not null into a database. 
\begin{lstlisting}
import sqlite3 as sql
class SportscrapePipeline(object):
	
	def open_spider(self,spider):
		self.db = sql.connect("test1")
		self.cur = self.db.cursor()
		self.cur.execute('DROP TABLE IF EXISTS vals')
		self.cur.execute('CREATE TABLE vals (name TEXT, numbers TEXT);')
		self.statement = "INSERT INTO vals VALUES(?, ?);"
		return

	def close_spider(self, spider):
		self.db.commit() #save changes made in the transaction
		self.db.close()
		return

	def process_item(self, item, spider):
		#data=(item['names'][0],item['numbers'][0])
		if item['names'] and item['numbers']:
			data=(item['names'][0],item['numbers'][0])
			self.cur.execute(self.statement, data)
		elif item['names'] and  not item['numbers']:
			data=(item['names'][0],"null")
			self.cur.execute(self.statement, data)
		elif not item['names'] and item['numbers']:
			data=("null",item['numbers'][0])
			self.cur.execute(self.statement, data)
\end{lstlisting}

\subsection*{Activating your Pipeline}
Once 